{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6Ic2axqvxDl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import scipy.sparse as sp\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R3zN3qzvY75Q"
   },
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def load_data(path=\"./data/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges_ordered_flatten = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    adj_unnormalize = adj.copy() # copy the unnormalized adjacency matrix\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(270)\n",
    "    idx_val = range(270, 2707)\n",
    "    idx_test = range(2200, 2707)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test, adj_unnormalize\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5900,
     "status": "ok",
     "timestamp": 1556488670974,
     "user": {
      "displayName": "kaikai zhao",
      "photoUrl": "",
      "userId": "02638836467976844998"
     },
     "user_tz": 240
    },
    "id": "Mvi8_qyQY75T",
    "outputId": "7045c17c-97d3-4e77-ba47-b2a523647e7a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# I use Kipf's load_data() and modified it by returning an unnormalized adjacency matrix\n",
    "adj, features, labels, idx_train, idx_val, idx_test, adj_unnormalize = load_data('')\n",
    "adj_array = adj_unnormalize.toarray() # transform csr to dense array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lGcBRtNVY75c"
   },
   "outputs": [],
   "source": [
    "# Build modularity matrix. For this part, please refer to Section 2.2 of [1]\n",
    "# k_i: the degree of vertex i; total number of edges: m = 1/2 * sum_i(k_i)\n",
    "# (k_i*k_j)/(2*m) describes the expected number of edges between vertices i and j if edges are placed randomly\n",
    "# Modularity matrix b_ij = a_ij - (k_i*k_j)/(2*m)\n",
    "degrees = np.array(adj_array.sum(1)) # calculate the degree of each node\n",
    "degrees = degrees[:,np.newaxis] # shape: V * 1\n",
    "m = 1/2 * degrees.sum() # the total number of edges in the network\n",
    "B = adj_array - 1/(2*m) * degrees.dot(degrees.T) # Modularity matrix\n",
    "B_tensor = torch.from_numpy(B) # convert numpy array into torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ypfRowkWY75h"
   },
   "outputs": [],
   "source": [
    "# This loss function adds the AutoEncoder loss and the pairwise loss together.\n",
    "# lamd is a tradeoff hyperparameter.\n",
    "class Semi_Loss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, lamd=0.01):\n",
    "        super(Semi_Loss,self).__init__()\n",
    "        self.lamd = lamd\n",
    "        \n",
    "    def forward(self, AE_loss, semi_loss):\n",
    "        semi_loss = AE_loss + self.lamd * semi_loss\n",
    "        return semi_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rrJLv48GY75k"
   },
   "outputs": [],
   "source": [
    "# build Laplace matrix. For this part, please refer to Section 4 of [1]\n",
    "# L = D - O\n",
    "labels_train = encode_onehot(labels[idx_train].numpy())\n",
    "n = idx_train.numpy().size\n",
    "O = labels_train.dot(labels_train.T) - np.eye(n) # O denotes pairwise constraint matrix\n",
    "L = np.diag(O.sum(1)) - O # L = D - O, D is a diagonal matrix whose entries are row summation of O\n",
    "L = torch.from_numpy(L).float() # convert numpy array into torch float tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTepDRfzY75n"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "# calculate normalized mutual info to measure the clustering performance\n",
    "def cal_nmi(data, labels, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++').fit(data)\n",
    "    pred_tr = kmeans.labels_\n",
    "    nmi = normalized_mutual_info_score(labels, pred_tr, average_method='arithmetic')\n",
    "    return nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "la263kApY75s"
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, output_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_size, input_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Train each autoencoder individually\n",
    "        x = x.detach()\n",
    "        y = self.encoder(x)\n",
    "            \n",
    "        return y.detach(), y \n",
    "\n",
    "    def reconstruct(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "class StackedAutoEncoder(nn.Module):\n",
    "    r\"\"\"\n",
    "    A stacked autoencoder made from the convolutional denoising autoencoders above.\n",
    "    Each autoencoder is trained independently and at the same time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(StackedAutoEncoder, self).__init__()\n",
    "\n",
    "        self.AE1 = AutoEncoder(2708, 512)\n",
    "        self.AE2 = AutoEncoder(512, 256)\n",
    "        self.AE3 = AutoEncoder(256, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1,h1 = self.AE1(x) # a1 has been detached. h1 can be used for backpropagation\n",
    "        a2,h2 = self.AE2(a1)\n",
    "        a3,h3 = self.AE3(a2)\n",
    "        \n",
    "        return h1, h2, h3, self.reconstruct(a3)\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "            a2_reconstruct = self.AE3.reconstruct(x)\n",
    "            a1_reconstruct = self.AE2.reconstruct(a2_reconstruct)\n",
    "            x_reconstruct = self.AE1.reconstruct(a1_reconstruct)\n",
    "            return x_reconstruct, a1_reconstruct, a2_reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_L1wQR0xRmx"
   },
   "outputs": [],
   "source": [
    "B_tensor = B_tensor.cuda()\n",
    "L = L.cuda()\n",
    "labels = labels.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0e9JW2zSY75y",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lamd = 5*10**-8\n",
    "model = StackedAutoEncoder()\n",
    "model = model.cuda()\n",
    "labels = labels.cpu()\n",
    "criterion = Semi_Loss(lamd)\n",
    "criterion_AE = nn.MSELoss()\n",
    "# criterion_AE = CrossEntropy_Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=10**-9)#10**-6\n",
    "epochs = 1000\n",
    "totalloss_list=[];pairwise_list=[];AE_list=[];\n",
    "nmi_train1=[]; nmi_train2=[]; nmi_train3=[];\n",
    "nmi_val1=[]; nmi_val2=[]; nmi_val3=[];\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    B_tensor = Variable(B_tensor).cuda()\n",
    "    H1, H2, H3, M = model(B_tensor) # H denotes low-embedding vectors, M is the reconstruction matrix\n",
    "    # For pairwise_loss, we only need to compute those which have labels.\n",
    "#     Our goal is to learn the low embedding representations of all vertices via a small portion of vertices with labels\n",
    "    pairwise1 = torch.mm( torch.mm(H1[idx_train,:].t(), L), H1[idx_train,:] )# H'*L*H\n",
    "    pairwise2 = torch.mm( torch.mm(H2[idx_train,:].t(), L), H2[idx_train,:] )\n",
    "    pairwise3 = torch.mm( torch.mm(H3[idx_train,:].t(), L), H3[idx_train,:] )\n",
    "    pairwise_loss = torch.trace(pairwise1) + torch.trace(pairwise2) + torch.trace(pairwise3)# trace(H'*L*H)\n",
    "#     pairwise_loss = torch.trace(pairwise3)\n",
    "    AE_loss = criterion_AE(M[0], B_tensor) + criterion_AE(M[1], H1) + criterion_AE(M[2], H2)# compute the reconstruction loss of AE\n",
    "    total_loss = criterion(AE_loss, pairwise_loss) # total loss= recon_loss + lamd*pairwise_loss\n",
    "    totalloss_list.append(total_loss.item()); pairwise_list.append(pairwise_loss.item()); AE_list.append(AE_loss.item())\n",
    "    if epoch%10 == 0:\n",
    "        model.eval()\n",
    "        H1, H2, H3, M = model(B_tensor)\n",
    "        H1, H2, H3 = H1.cpu(), H2.cpu(), H3.cpu()\n",
    "        nmi_train_H1 = cal_nmi(data=H1[idx_train.numpy(),:].detach().numpy(), labels=labels[idx_train.numpy()], n_clusters=7)\n",
    "        nmi_train_H2 = cal_nmi(data=H2[idx_train.numpy(),:].detach().numpy(), labels=labels[idx_train.numpy()], n_clusters=7)\n",
    "        nmi_train_H3 = cal_nmi(data=H3[idx_train.numpy(),:].detach().numpy(), labels=labels[idx_train.numpy()], n_clusters=7)\n",
    "        nmi_val_H1 = cal_nmi(data=H1[idx_val.numpy(),:].detach().numpy(), labels=labels[idx_val.numpy()], n_clusters=7)\n",
    "        nmi_val_H2 = cal_nmi(data=H2[idx_val.numpy(),:].detach().numpy(), labels=labels[idx_val.numpy()], n_clusters=7)\n",
    "        nmi_val_H3 = cal_nmi(data=H3[idx_val.numpy(),:].detach().numpy(), labels=labels[idx_val.numpy()], n_clusters=7)\n",
    "        print('epoch{}:total_loss:{:.6f}|AE_loss: {:.6f}|pair_loss: {:.6f}|NMI train(H1 H2 H3): '\n",
    "              '{:.2f} {:.2f} {:.2f}|val: {:.2f} {:.2f} {:.2f}'.format\n",
    "              (epoch, total_loss, AE_loss, pairwise_loss, nmi_train_H1, nmi_train_H2, nmi_train_H3, nmi_val_H1,\n",
    "              nmi_val_H2, nmi_val_H3))\n",
    "        model.train()\n",
    "        nmi_train1.append(nmi_train_H1); nmi_train2.append(nmi_train_H2); nmi_train3.append(nmi_train_H3)\n",
    "        nmi_val1.append(nmi_val_H1); nmi_val2.append(nmi_val_H2); nmi_val3.append(nmi_val_H3)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DN6JYPsuygwF"
   },
   "outputs": [],
   "source": [
    "np.savez('im22-data',epochs=epochs,totalloss_list=totalloss_list,AE_list=AE_list,pairwise_list=pairwise_list,\n",
    "         nmi_train1=nmi_train1,nmi_train2=nmi_train2,nmi_train3=nmi_train3,\n",
    "         nmi_val1=nmi_val1,nmi_val2=nmi_val2,nmi_val3=nmi_val3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rdWQEJqKY757"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import matplotlib.pyplot as plt\n",
    "host = host_subplot(111)\n",
    "par = host.twinx()\n",
    "host.set_xlabel(\"Epochs\")\n",
    "host.set_ylabel(\"Total-loss\")\n",
    "par.set_ylabel(\"Pairwise-loss\")\n",
    "p1, = host.plot(range(epochs), totalloss_list, 'r-', lw=2, label=\"Total-loss\")\n",
    "p2, = host.plot(range(epochs), AE_list, 'k-', lw=2, label=\"AE-loss\")\n",
    "p3, = par.plot(range(epochs), np.array(pairwise_list), 'b-', lw=2, label=\"Pairwise-loss\")\n",
    "leg = plt.legend(loc='best')\n",
    "host.yaxis.get_label().set_color(p1.get_color())\n",
    "leg.texts[0].set_color(p1.get_color())\n",
    "par.yaxis.get_label().set_color(p3.get_color())\n",
    "leg.texts[2].set_color(p3.get_color())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dEZrDxaCY75_"
   },
   "outputs": [],
   "source": [
    "plt.plot( nmi_train1, 'r-', lw=2, label='nmi_train1')\n",
    "plt.plot( nmi_train2, 'y-', lw=2, label='nmi_train2')\n",
    "plt.plot( nmi_train3, 'g-', lw=2, label='nmi_train3')\n",
    "plt.plot( nmi_val1, 'r--', lw=2, label='nmi_val1')\n",
    "plt.plot( nmi_val2, 'y--', lw=2, label='nmi_val2')\n",
    "plt.plot( nmi_val3, 'g--', lw=2, label='nmi_val3')\n",
    "# plt.plot(range(epochs), np.array(pairwise_list)*lamd, 'b-', lw=3, label='pairwise_loss')\n",
    "plt.legend(loc='upper left');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ns-OCZYiynum"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "modularity-based-stacked-autoencoder-CD.ipynb",
   "provenance": [
    {
     "file_id": "1dxURYNy5mO5ugdEFzbOugrM543iB270M",
     "timestamp": 1556050205630
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
